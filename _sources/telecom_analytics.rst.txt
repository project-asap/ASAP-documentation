.. highlight:: rst

***************************
Telecommunication Analytics
***************************

Introduction
############


The goal of WIND was to design and implement a new application to take better advantage of the new big data approach for mobile applications. The ASAP telecommunications  application  will  show  how  a  number  of  analytical  services describing the mobility of people can be created on the basis of the data collected by the mobile network during routine operation. We developed three modules: the user profiling, the sociometer and the peak detection.   Moreover  some  additional  modules  are  defined and  developed  in  order  to  build  complete  workflows  going  from  the  data  to  the publication of the results, as shown in the figure below. 

.. image:: figures/WIND_general_modules.png



Peak Detection
##############

This analysis detects relevant peaks representing an event. This is achieved by comparing the density of population (measured in calls) within a region in a given moment against the expected density for that area at that hour of the day.

The overall analysis is decomposed in the following operators:

    Data Filter
    Distribution Computation
    Peak Detection

The above operators are implemented as Spark applications (using the scala API) and they can be executed by submitting them to a running Spark installation.

The overall calculation is illustrated here:


.. image:: figures/PeakDetection.png



Data Filter
***********

This is the first step of the Peak Detection calculation.

The process expects a CDR dataset containing the following fields:

    1. caller id (masked)
    2. call date (format: yyyy-MM-dd)
    3. chargable duration
    4. tower identifier from which the call has initiated
    5. tower identifier where the call has ended

Then, the process assumes a dataRaw dataset derived from the above and containing the following fields:

    • id : tower identifier from which the call has initiated
    • hour: the hour of the day (derived by the call date)
    • dow: the day of the week (derived by the call date)
    • doy: the day of the year (derived by the call date)
    • num: the number of calls started in this tower range at this hour of this specific day.

The next step of the process consists in defining the geographical area to analyze and to partition it into a set of regions. The same must be done for the time, where a timeframe is chosen (for instance, a month), partitioned into periods (for instance, days) and then into smaller time slots (for instance, hours). Time slots are described by a parameter T, while the regions that cover the area of analysis are described by a parameter S, both parameters being provided by the user. These two parameters, then, allow defining a spatio-temporal grid, and each observation of an input dataset can be assigned to one of its cells. The number of observations that fall in a cell defines its density. The input data is partitioned into two sets: a training dataset and a test dataset. For both datasets the spatiotemporal grid of densities is computed. The first will be used to compute the densities of a typical period for each region. The second dataset will be then compared against such typical period in order to detect significant deviations.


Implementation Details
----------------------


As an spark application, this operator can be executed by being submitted in an running spark installation. For simplifying the execution submit.sh can be used.

Usage: ./submit.sh ta.DataFilter <master> <cdrPath> <output> <trainingSince (yyyy-MM-dd)> <trainingUntil (yyyy-MM-dd)> <testSince (yyyy-MM-dd)> <testUntil (yyyy-MM-dd or None)> <voronoiPath>

**Input parameters**:

    1. The spark master URI
    2. The input CDR dataset (HDFS or local)
    3. The ouput path (an non existing HDFS or local directory)
    4. The start date for the training period (format yyyy-MM-dd)
    5. The end date for the training period (format yyyy-MM-dd)
    6. The start date for the test period (format yyyy-MM-dd)
    7. The end date for the training period (format yyyy-MM-dd) or None
    8. The path to the voronoi table: the set of towers ids in analysis.

**Output**: Upon successful execution the <output>/trainingData & <output>/testData datasets will be created.

e.g.: ./submit.sh ta.DataFilter spark://localhost:7077 /dataset_simulated /output 2015-06-01 2015-06-02 2015-06-03 None /voronoi



Distribution Computation
************************

This is the second step of the Peak Detection calculation and it requires input generated by the Data Filter step.

Based on the densities obtained for each region and each time slot over the training dataset, an expected density value is computed for each region, by averaging the densities measured at the same time slot of all the periods in the time window covered by the dataset. For instance, we might obtain an expected density for each pair (region, hour of the day), i.e., 24 values for each region, assuming 24 one-hour time-slots. The result represents the standard behavior and it is saved in a new dataset named cpBase.


Implementation Details
-----------------------

As an spark application, this operator can be executed by being submitted in an running spark installation. For simplifying the execution submit.sh can be used.

Usage: ./submit.sh ta.DistributionComputation <master> <trainingDataFile> <output>

**Input parameters**:

    1. The spark master URI
    2. The training dataset URI (HDFS or local) (created during the data filtering step)
    3. The ouput path (an non existing HDFS or local directory)

**Output**: Upon successful execution the <output>/cpBase dataset will be created.

e.g.: ./submit.sh ta.DistributionComputation spark://localhost:7077 /output/trainingData /output





