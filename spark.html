<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Extension of the Apache Spark &#8212; Asap 0.0 documentation</title>
    
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="top" title="Asap 0.0 documentation" href="index.html" />
    <link rel="next" title="Analytics Programming Model - SWAN Compiler" href="swan.html" />
    <link rel="prev" title="Workflow and Operator Definition" href="ires_docs/implementation.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="extension-of-the-apache-spark">
<h1>Extension of the Apache Spark<a class="headerlink" href="#extension-of-the-apache-spark" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>We extended the Apache Spark programming model and scheduler to supoort nested RDD operations, to facilitate expressing recursive and hierarchical computations. Apache Spark is a fast and general cluster computing system for Big Data that uses RDD abstractions. RDDs are immutable partitioned collections that stored in a storage system such as HDFS or derived by applying operators to other RDDs. We also modified the default Spark scheduling mechanism to support many schedulers and not only a central master scheduler.</p>
<div class="code bash highlight-rst"><div class="highlight"><pre><span></span>val file1 = sc.textFile(&quot;hdfs://file1&quot;)
val file2 = sc.textFile(&quot;hdfs://file2&quot;)
file1.map(word1 =&gt;
        file2.filter(word2 =&gt;
                (word1.length &gt; word2.length))
                        .collect())
                .collect()
</pre></div>
</div>
<p>Let see the above example code that creates two RDDs froma two HDFS files and performs a map operation on RDD file1. The mapper function of the map operation performs a filter operation on RDD file2 for every word in RDD file1 to select all the words that have the larger length. The collect() function forces the computation and collect the results into an array. By default, Apache Spark does not support such nested RDD operations. Our extension handles nested RDD operators and requires from the executor nodes to behave as the master node. In this example, the scheduler creates tasks that execute the mapper function and distribute it to the executors, then it creates a task for every partition of the file1 RDD and sends each task to an idle executor to run the mapper function on that partition. In the executor nodes, when the mapper function runs, it tries to invoke a filter operation on the file2 RDD. We extended the executor functionality to capture this event and send a CreateRDD message to the scheduler node. The message contains an identifier of the RDD object referenced, the (reflective) name of the invoked operation, and a serialized version of the user-defined function that is applied. We also extended the Spark scheduler to receive messages from the executors. Upon receiving such a forwarded RDD operation message, the scheduler looks up the RDD with the specified id and, invokes the specified operation.</p>
</div>
<div class="section" id="link">
<h2>Link<a class="headerlink" href="#link" title="Permalink to this headline">¶</a></h2>
<p>The code of the NestedSpark can be found in</p>
<div class="code bash highlight-rst"><div class="highlight"><pre><span></span>https://github.com/project-asap/spark01.git
</pre></div>
</div>
</div>
<div class="section" id="install">
<h2>Install<a class="headerlink" href="#install" title="Permalink to this headline">¶</a></h2>
<p>For demostration reasons a Linux operating system like Ubuntu it is assumed in this step. In Windows or other Linux distributions the equivalents should be done.</p>
<p>The build instructions are the following:</p>
<ol class="arabic simple">
<li>Install sbt:</li>
</ol>
<div class="code bash highlight-rst"><div class="highlight"><pre><span></span>sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823
sudo apt-get update
sudo apt-get install sbt
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li>Check the version of the installed Hadoop-yarn</li>
<li>Build the spark:</li>
</ol>
<div class="code bash highlight-rst"><div class="highlight"><pre><span></span>cd &lt;Spark_home&gt;
./build/sbt -Dhadoop.version=&lt;Hadoop_version&gt; -Pyarn -DskipTests clean assembly
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li>Configure spark:</li>
</ol>
<div class="code bash highlight-rst"><div class="highlight"><pre><span></span>cd &lt;Spark_home&gt;
cp conf/spark-env.sh.template conf/spark-env.sh
</pre></div>
</div>
<p>Set the IP of master node in the file conf/spark-env.sh</p>
<div class="code bash highlight-rst"><div class="highlight"><pre><span></span>cp conf/spark-defaults.conf.template conf/spark-defaults.conf
</pre></div>
</div>
<p>For every slaves node insert the list of all executor&#8217;s IPs</p>
<p>For every slaves node execute: cp conf/slaves.template conf/slaves</p>
<ol class="arabic simple" start="5">
<li>Start spark:</li>
</ol>
<div class="code bash highlight-rst"><div class="highlight"><pre><span></span>./sbin/stop-all.sh
</pre></div>
</div>
</div>
<div class="section" id="tests">
<h2>Tests<a class="headerlink" href="#tests" title="Permalink to this headline">¶</a></h2>
<p>Clone the code of spark tests:</p>
<div class="code bash highlight-rst"><div class="highlight"><pre><span></span>git clone https://github.com/project-asap/spark-tests.git
</pre></div>
</div>
<ol class="arabic simple">
<li>Build In the &lt;Spark_tests_home&gt; execute:</li>
</ol>
<div class="code bash highlight-rst"><div class="highlight"><pre><span></span>mkdir -p /lib
cp &lt;Spark_home&gt;/assembly/target/scala-2.10/spark-assembly-*.jar lib/
sbt clean package
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li>Test the hierarchical, in the &lt;Spark_tests_home&gt;:</li>
</ol>
<div class="code bash highlight-rst"><div class="highlight"><pre><span></span><span class="nt">&lt;Spark_home&gt;</span>/bin/spark-submit --class HierarchicalKMeansPar target/scala-2.10/spark-tests_2.10-1.0.jar spark://&lt;Spark_master-ip&gt;:7077 100 2 2 2 &lt;text_file_path&gt; --dist-sched false
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li>Test the distributed scheduler, in the &lt;Spark_tests_home&gt;:</li>
</ol>
<div class="code bash highlight-rst"><div class="highlight"><pre><span></span><span class="nt">&lt;Spark_home&gt;</span>/bin/spark-submit --class Run target/scala-2.10/spark-tests_2.10-1.0.jar --master spark://&lt;Spark_master_ip&gt;:7077 --algo Filter33 --dist-sched true --nsched 4 --partitions 32 --runs 15
</pre></div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Extension of the Apache Spark</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#link">Link</a></li>
<li><a class="reference internal" href="#install">Install</a></li>
<li><a class="reference internal" href="#tests">Tests</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="ires_docs/implementation.html" title="previous chapter">Workflow and Operator Definition</a></li>
      <li>Next: <a href="swan.html" title="next chapter">Analytics Programming Model - SWAN Compiler</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/spark.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, Asap consortium.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.5a1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.9</a>
      
      |
      <a href="_sources/spark.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>